{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from skimage import io, transform\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torchvision.utils as vutils\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "std,mean = [0.229, 0.224, 0.225],[0.485, 0.456, 0.406] # pretrained models used these values\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean,\n",
    "                         std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img, title='', std=std, mean=mean,figsize=(16,16)):\n",
    "    img = img.permute(1,2,0).cpu()\n",
    "    img = img * torch.tensor(std) + torch.tensor(mean)\n",
    "    img = np.clip(img, 0, 1)\n",
    "    plt.figure (figsize = figsize)\n",
    "    plt.imshow(img)\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset with file name https://gist.github.com/andrewjong/6b02ff237533b3b2c554701fb53d5c4d\n",
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        path = self.imgs[index][0]\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path.cwd() / 'index_images'\n",
    "dataset = ImageFolderWithPaths(data_dir,transform=test_transform) # our custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=128\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=bs, shuffle=False,num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingExtractor:\n",
    "    def sniff_output(self,model, input, output):\n",
    "        self.embeddings=output  \n",
    "    def __init__(self):\n",
    "        self.model = models.resnet34(pretrained=False)\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, 251)\n",
    "        self.model = self.model.to(device)\n",
    "        self.model.load_state_dict(torch.load('model.pt'))\n",
    "        self.model.eval()\n",
    "        layer = self.model._modules.get('avgpool')\n",
    "        self.handle = layer.register_forward_hook(self.sniff_output)\n",
    "    def get_embeddings(self, input):\n",
    "        with torch.no_grad():\n",
    "            self.model(input.to(device))\n",
    "        return self.embeddings.squeeze(-1).squeeze(-1)\n",
    "extractor = EmbeddingExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = AnnoyIndex(512, 'euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58593da209a403682d9f9ba8e0b5aaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1715.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for batch in tqdm(dataloader):\n",
    "    embeddings = extractor.get_embeddings(batch[0])\n",
    "    for i in range(len(batch[2])):\n",
    "        emb = embeddings[i]\n",
    "        img_id = os.path.basename(batch[2][i]).split('.')[0]\n",
    "        t.add_item(int(img_id),emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.build(5) # 5 trees\n",
    "t.save('tree_5.ann')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
